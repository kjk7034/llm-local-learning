# LLM í•™ìŠµ

ìµœê·¼ AI ê´€ë ¨ í•™ìŠµì„ í•˜ë‹¤ë³´ë‹ˆ, APIë¥¼ í†µí•´ì„œ í´ë¼ìš°ë“œì— ë°ì´í„° ì „ì†¡ì´ ë³´ì•ˆì ìœ¼ë¡œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ë“¤ë„ ë§ê² ë‹¤ê³  ìƒê°ì„ í–ˆë‹¤.

ê·¸ë˜ì„œ ë¡œì»¬ í™˜ê²½ì—ì„œ ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ê³  ì‹¶ì–´ì„œ ì‹œì‘í–ˆë‹¤.

## Msty ì„¤ì¹˜ ë° ì‹¤í–‰

ë‹¤ì–‘í•œ ì œí’ˆì´ ìˆì—ˆë‹¤. LMStudio, ollama, Msty, Llama.cpp ë“±ë“±...

ì—¬ê¸°ì„œ íŒ€ì—ì„œ ê³µìœ ëœ Mstyë¥¼ ì„¤ì¹˜ ë° ì‹¤í–‰í–ˆë‹¤.

[Msty](https://msty.app/)ì— ì ‘ì†í•´ì„œ ë‹¤ìš´ë¡œë“œ í›„ ì‹¤í–‰í•˜ë©´ ë... ë„ˆë¬´ë‚˜ë„ ê°„ë‹¨í–ˆë‹¤.

<img src="./images/msty_01.png" alt="" />

ììœ ë¡­ê²Œ Local AI ëª¨ë¸ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤.

<img src="./images/msty_02.png" alt="" />

## Open WebUI (Formerly Ollama WebUI) ğŸ‘‹

ë¡œì»¬ì—ì„œ Web UIë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” LLMë¡œ ë˜ ë‹¤ë¥¸ Web UIë¥¼ í…ŒìŠ¤íŠ¸ í–ˆë‹¤.

[Download Ollama](https://ollama.com/download)ì—ì„œ ë‹¤ìš´ë¡œë“œ.

Dockerë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰.

```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

ì„¤ì¹˜ í›„ ëª¨ë¸ì€ ê´€ë¦¬ê°€ íŒ¨ë„ > ëª¨ë¸ > Ollama.comì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°(pull)ë¥¼ í†µí•´ì„œ ì„¤ì¹˜í•  ìˆ˜ ìˆì—ˆë‹¤.

<img src="./images/webui_01.png" alt="" />

## GGUF íŒŒì¼ ìƒì„±í•˜ê¸°

Mstyì—ëŠ” GGUF ëª¨ë¸ì„ importí•˜ëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤.

íšŒì‚¬ì—ì„œ [NCSOFT/Llama-VARCO-8B-Instruct](https://huggingface.co/NCSOFT/Llama-VARCO-8B-Instruct/tree/main)ë¥¼ ì˜¤í”ˆí–ˆë‹¤.

ë¡œì»¬ì— ì„¤ì¹˜í•˜ê³  ì‹¶ì–´ì„œ í™•ì¸í–ˆë”ë‹ˆ GGUF íŒŒì¼ì´ ì—†ì–´ì„œ í•´ë‹¹ íŒŒì¼ì„ ìƒì„±í•´ë´¤ë‹¤.

ì²˜ìŒì—ëŠ” HuggingFaceë¥¼ ì´ìš©í•´ì„œ ê°„ë‹¨í•˜ê²Œ ë³€ê²½í•˜ê³  ì‹¶ì—ˆìœ¼ë‚˜ `Llama-VARCO-8B-Instruct`ê°€ LLaMA 3 ë² ì´ìŠ¤ì—¬ì„œ ì§ì ‘ gguf ëª¨ë¸ì„ ìƒì„±í•˜ê¸°ë¡œ í–ˆë‹¤.

### ìƒì„± ê³¼ì •

#### Downloading a HuggingFace model

```python
pip install huggingface_hub
```

`download.py` íŒŒì¼ ìƒì„±

```python
# download.py íŒŒì¼
from huggingface_hub import snapshot_download
model_id="NCSOFT/Llama-VARCO-8B-Instruct"
snapshot_download(repo_id=model_id, local_dir="llama-varco",
                  local_dir_use_symlinks=False, revision="main")
```

`download.py` íŒŒì¼ ì‹¤í–‰

```python
python download.py
```

`ls -lash llama-varco` ë‹¤ìš´ë¡œë“œ í™•ì¸

#### Converting the model

`llama.cpp` clone

```
git clone https://github.com/ggerganov/llama.cpp.git
```

íŒŒì´ì¬ ì˜ì¡´ì„± ì„¤ì¹˜

```python
pip install -r llama.cpp/requirements.txt
```

íŠœí† ë¦¬ì–¼ì—ëŠ” `llama.cpp/convert.py`ì„ ì‹¤í–‰ì‹œì¼°ì§€ë§Œ, Llama 3ë¥¼ ë³€í™˜ì‹œí‚¤ê¸° ìœ„í•´ì„œ `convert_hf_to_gguf.py`ì„ ì ìš©.

```python
python llama.cpp/convert_hf_to_gguf.py llama-varco --outfile Llama-VARCO-8B-Instruct.gguf --outtype q8_0
```

ì„¤ì¹˜ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒ. (`WARNING: The BPE pre-tokenizer was not recognized! .... chkhsh: xxxxx`)

`convert_hf_to_gguf.py`íŒŒì¼ì— `get_vocab_base_pre` í•¨ìˆ˜ ë‚´ì— `llama-bpe` ì¼€ì´ìŠ¤ ì¶”ê°€

```python
 if chkhsh == "xxxxx":
    # ref: https://huggingface.co/meta-llama/Meta-Llama-3-8B
    res = "llama-bpe"
```

ìƒì„± ì™„ë£Œ.

### GGUF íŒŒì¼ ì ìš©í•˜ê¸°.

ìƒì„±ëœ ggufë¥¼ ë¡œì»¬ AIì— Import

<img src="./images/msty_03.png" alt="" />

ì‹¤í–‰.

<img src="./images/msty_04.png" alt="" />

## ì°¸ê³ ìë£Œ

- [Quick Start with Docker](https://github.com/open-webui/open-webui?tab=readme-ov-file#quick-start-with-docker-)ë¥¼ ì§„í–‰í—€ë‹¤.
- [Tutorial: How to convert HuggingFace model to GGUF format](https://github.com/ggerganov/llama.cpp/discussions/2948)
